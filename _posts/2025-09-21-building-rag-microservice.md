---
title: "Building a Privacy-First RAG Microservice with FastAPI & Open Source Tools"
categories:
  - Backend
  - Microservices
  - Genai
  - RAG
tags:
  - fastapi
  - llm
  - chroma
  - vllm
  - minio
  - huggingface
  - privacy
layout: single
---

> â€œI didnâ€™t want to just *use* AI â€” I wanted to *understand* it.â€

Thatâ€™s why I built my own **RAG (Retrieval-Augmented Generation) microservice** from scratch using **FastAPI** and entirely **open-source tools** â€” no Bedrock, no OpenAI API, no managed services. Just raw, hands-on learning.

This project was born out of curiosity and a desire to peel back the layers of modern LLM-powered applications. I wanted to know: *Whatâ€™s really happening under the hood?* And more importantly â€” *how can I keep my data private while still leveraging powerful models?*

---

## ğŸ§° The Tech Stack

Hereâ€™s what I wired together:

- **MinIO** â†’ Self-hosted S3-compatible storage for document uploads
- **Chroma** â†’ Lightweight, open-source vector database for storing embeddings
- **vLLM** â†’ High-throughput LLM inference engine (for future chat integration)
- **Hugging Face Text Embeddings Inference** â†’ Fast, scalable embedding generation
- **FastAPI** â†’ The glue holding it all together with clean, async endpoints

All containerized via Docker and orchestrated with `docker-compose.yml` â€” because if itâ€™s not reproducible, did it even happen?

---

## ğŸš€ Endpoints I Built

```bash
POST   /upload      â†’ Upload file â†’ stores in MinIO
DELETE /delete/{filename} â†’ Delete file from MinIO
POST   /embed       â†’ Generate embeddings for ALL files in MinIO
POST   /embed/{filename} â†’ Embed a specific file
POST   /retrieve    â†’ Retrieve top-k relevant chunks for a query
```

Simple? Yes.  
Powerful? Absolutely.

Each endpoint is designed to be modular, testable, and â€” most importantly â€” *understandable*. No black boxes.

---

## ğŸ§  What I Learned (The Hard Way)

Building RAG from scratch revealed surprising complexities:

### 1. File-Embedding Synchronization

What happens when you rename or delete a file? Do you delete its embeddings? How do you track which embeddings belong to which file version? I learned the hard way that **random hashes donâ€™t cut it** â€” you need **deterministic hashing** (e.g., based on filename + content hash) to maintain consistency.

### 2. The Need for an Embedding Registry

I didnâ€™t realize how crucial it is to maintain a **metadata registry** mapping files â†’ embedding IDs â†’ chunk IDs. Without it, youâ€™re flying blind when files change or models get updated.

### 3. Pipeline Rebuilds Are Inevitable

Switching embedding models? Updating chunking strategies? You need a **full re-embedding pipeline** â€” with evaluation metrics to measure drift or performance loss. RAG isnâ€™t â€œset and forget.â€ Itâ€™s a living system.

---

## ğŸ” Why Privacy Matters (And Why I Built This)

Letâ€™s be honest: **most LLM services keep your data**. Whether itâ€™s for training, telemetry, or â€œimproving the product,â€ your documents, queries, and context are rarely truly private.

I built this so I â€” and you â€” can:

âœ… Use local files  
âœ… Avoid sending data to third parties  
âœ… Swap models freely (Llama, Mistral, Phi, etc.)  
âœ… Own the entire stack â€” from ingestion to response

This isnâ€™t just a toy project. Itâ€™s a **blueprint for private, auditable, enterprise-ready RAG**.

---

## ğŸ—ºï¸ Whatâ€™s Next?

Iâ€™m now integrating **vLLM for chat endpoints** â€” letting users query their documents and get LLM-generated responses â€” all locally.

Future plans:

- Experiment with **different RAG architectures** (HyDE, sub-queries, re-ranking)
- Add **evaluation metrics** (hit rate, MRR, faithfulness)
- Support **multiple file types** (PDF, DOCX, PPTX, images via OCR)
- Build a **simple frontend** (maybe with Streamlit or React)

---

## ğŸ“ Sneak Peek: Project Structure

For the curious (and fellow nerds), hereâ€™s how I organized the codebase:

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ routers/          # FastAPI route handlers
â”‚       â”œâ”€â”€ services/         # Business logic
â”‚       â”‚   â”œâ”€â”€ embeddings/   # Document loading, chunking, embedding, vector storage
â”‚       â”‚   â””â”€â”€ llm/          # Future LLM integration
â”‚       â”œâ”€â”€ schemas/          # Pydantic models
â”‚       â”œâ”€â”€ domain/           # Protocols
â”‚       â”œâ”€â”€ core/             # Config
â”‚       â””â”€â”€ utils/            # Helpers (logging, ID generation)
â””â”€â”€ uploads/                  # Local file storage (mounted to MinIO)
```

Clean separation of concerns. Testable components. Extensible design.

---

## ğŸ’¬ Final Thoughts

This project taught me more than any tutorial or course ever could. Thereâ€™s something deeply satisfying about wiring together open-source tools, debugging tokenizer mismatches, and finally seeing your RAG system return a relevant chunk from your own document.

If youâ€™re thinking about dipping your toes into RAG â€” **donâ€™t reach for an API first**. Build it yourself. Break it. Fix it. Youâ€™ll learn 10x more.

---

ğŸ”— **GitHub Repo**: [github.com/debabrot/intelligent-doc-assistant](https://github.com/debabrot/intelligent-doc-assistant)  

---
